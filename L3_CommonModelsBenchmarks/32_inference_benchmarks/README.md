# DAIBench Inference Benchmark

## 1. Introduction

DAIBench Inference Benchmark is made above [Nvidia TensorRT](https://developer.nvidia.com/tensorrt). 

You can measure the inference performance of different GPU(brand covers Geforce/Quadro/Tesla, arch covers Pascal, Volta, Turing, Ampere etc), different models(mainly CNN), different precisions(FP32, FP16, INT8, etc) and different batch sizes(from 1, 2, 4 upto 128) with this tool.

## 2. Installation

Highly recommend [NGC environment](https://catalog.ngc.nvidia.com/). 

This tool is compatible with NGC 22.04. Newer version or older version may be incorrect in results' format, you need modify something for better view effects.

```
docker run -ti --shm-size 4G -v /data:/data nvcr.io/nvidia/tensorflow:22.04-tf1-py3
git clone git@git.xiaojukeji.com:MachineLearning/daibench.git
cd daibench/models/inference/
./autorun.sh
```

This may take a few hours to finish. Take a cup of coffee now.

## 3. Results

If all is well, you may see a folder with name YYMMDDhhmmss_logs, a lot of log files are placed here. The results are so sophisticated that we cannot read them one by one.

You need this tool for visualizing results:

```
./show_results.sh YYMMDDhhmmss_logs
```

Results may be readable now:

```
===================alexnet====================
Batch	FP32	TF32	FP32*	TF32*	FP16	INT8
1	1388.27	1489.12	1486.74	1658.71	2760.36	3356.93
2	2073.51	2642.55	2171.11	2976.2	5204.84	6363.8
4	3573.17	4939.2	3738.08	5390.76	9326.33	11374.6
8	5643.19	7393.54	5835.8	8004.66	14179.9	18828.5
16	7421.71	10732.4	7557.28	11261.8	19605.9	27717.2
32	9146.94	13926.7	9234.4	14085.7	24412.1	31132.5
64	10688.5	16291.7	10718.8	16439	27812.9	34698.1
128	11536.5	18347.6	11551	18396.9	30283.5	36391.4
===================googlenet====================
Batch	FP32	TF32	FP32*	TF32*	FP16	INT8
1	1309.67	1298.95	1404.17	1431.74	2272.37	2989.09
2	1532.2	1657.5	1650.06	1969.04	4069.01	5381.31
4	2031.45	2067.1	2124.32	2900.83	6359.32	9200.79
8	2834.02	3658.19	2945.04	3916.08	8753.13	14281.3
16	3482.88	4549.7	3556.77	4726.8	10528.1	19615.5
32	3966.58	5289.81	3990.08	5419.24	11559	23741.6
64	4216.78	5737.15	4215.89	5805.6	12593.2	26957.3
128	4289.9	6085.75	4297.43	6125.48	13093.1	28968.9
===================resnet18====================
Batch	FP32	TF32	FP32*	TF32*	FP16	INT8
1	1115.16	1323.91	1176.17	1431.3	3713	5329.34
2	1724.08	2215.9	1825.39	2401.04	6661.9	9201.45
4	2907.48	3635.34	3014.58	3870.18	9690.13	14747.1
8	3728.76	5159.25	3825.4	5385.23	12551.2	20238.7
16	4159.02	6505.15	4195.82	6714.12	14284.3	27281.5
32	4446.45	7545.52	4467.92	7618.67	15454.9	31588.6
64	4952.71	8100.65	4940.93	8155.04	16821.4	35574.8
128	5241.61	8518.89	5242.78	8568.35	17792	37517.2
===================resnet50====================
Batch	FP32	TF32	FP32*	TF32*	FP16	INT8
1	654.343	694.657	676.506	748.807	1982.06	2853.19
2	875.384	1044.32	915.687	1149.81	3363.17	5017.87
4	1317.95	1615.17	1359.71	1723.58	4751.75	7656.53
8	1649.81	2270.02	1678.65	2344.03	5682.28	10661.2
16	1862.13	2906.7	1871.7	2954.45	6284.86	13182
32	1971.14	3304.56	1977.8	3348.52	6988.11	14991.8
64	2178.12	3637.52	2170.72	3656.25	7601.76	16593
128	2225.11	3852.66	2216.86	3869.21	7983.68	17677.6
===================resnet101====================
Batch	FP32	TF32	FP32*	TF32*	FP16	INT8
1	327.669	364.337	345.081	390.188	1102.33	1569.97
2	443.103	532.272	463.659	629.172	1873.61	2776.97
4	704.252	882.628	728.041	936.001	2771.2	4371.18
8	876.89	1178.46	893.095	1228.85	3299.01	6213.64
16	979.024	1689.44	986.669	1733.86	3621.27	7541.72
32	1058.25	1879.51	1060.48	1900.93	4102.07	8646.97
64	1198.96	2155.52	1198.16	2167.07	4503.21	9606.48
128	1245.46	2329.9	1238.87	2337.5	4792.06	10331.2
===================resnet152====================
Batch	FP32	TF32	FP32*	TF32*	FP16	INT8
1	224.727	251.934	237.373	269.553	761.039	1109.87
2	303.203	347.484	317.685	434.706	1301.03	1895.47
4	480.333	604.435	498.59	644.422	1942.33	3078
8	592.981	810.166	603.914	846.966	2289.26	4251.06
16	662.834	1179.25	669.534	1202.93	2527.64	5163.41
32	718.694	1296.72	716.026	1314.17	2848.3	5929.1
64	810.042	1499.49	813.202	1510.78	3136.09	6584.6
128	845.335	1628.05	841.601	1632.65	3342.24	7067.31
===================vgg16====================
Batch	FP32	TF32	FP32*	TF32*	FP16	INT8
1	408.242	492.085	416.835	501.176	1136.06	2153.95
2	475.825	679.52	481.117	688.863	1644.09	3285.71
4	563.263	863.776	565.276	871.822	2144.51	4298.66
8	621.342	1033.83	622.608	1047.06	2579.48	5327.71
16	650.929	1130.52	650.702	1134.83	2899.64	5921.37
32	690.939	1217.28	695.386	1217.06	3108.12	6414.79
64	792.449	1246.34	793.358	1247.86	3230.31	6618.16
128	777.536	1242.16	777.844	1239.72	3293.79	6734.75
===================vgg19====================
Batch	FP32	TF32	FP32*	TF32*	FP16	INT8
1	345.002	428.073	350.636	436.532	1002.68	1898.93
2	396.958	585.924	397.284	593.007	1412.2	2839.93
4	460.13	734.477	462.475	740.977	1808.3	3622.3
8	504.475	887.748	505.337	890.155	2154.07	4469.69
16	524.462	962.502	523.505	964.94	2398.09	4900.95
32	563.995	1030.27	564.649	1027.67	2551.09	5260.96
64	642.121	1052.13	642.262	1053.08	2646.97	5430.78
128	621.824	1035.92	623.109	1042.07	2691.92	5531.19

Note: * means with cudagraph
```

You can paste them from terminal to excel files, and draw diagrams for better visual effects.



